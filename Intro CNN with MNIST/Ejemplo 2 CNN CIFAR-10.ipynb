{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Una red neuronal convolucional para detectar objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import urllib\n",
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "output_every = 50\n",
    "generations = 20000\n",
    "eval_every = 500\n",
    "image_height = 32\n",
    "image_width = 32\n",
    "crop_height = 24\n",
    "crop_width = 24\n",
    "num_channels = 3\n",
    "num_targets = 10\n",
    "data_folder = \"cifar-10-batches-bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "lr_decay = 0.9\n",
    "num_gens_to_wait = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vect_length = image_height*image_width*num_channels\n",
    "record_length = 1+image_vect_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descarga y procesamiento de CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'cifar-10-temp'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "cifar10_url = \"http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\"\n",
    "data_file = os.path.join(data_dir, 'cifar-10-binary.tar.gz')\n",
    "if not os.path.isfile(data_file):\n",
    "    filepath, _ = urllib.request.urlretrieve(cifar10_url, data_file)\n",
    "    tarfile.open(filepath, 'r:gz').extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cifar_files(filename_queue, distort_images = True):\n",
    "    reader = tf.FixedLengthRecordReader(record_bytes=record_length)\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    #creamos fichero binario\n",
    "    record_bytes = tf.decode_raw(record_string, tf.uint8)\n",
    "    #extraemos la etiqueta\n",
    "    image_label = tf.cast(tf.slice(record_bytes, [0], [1]), tf.int32)\n",
    "    #extraemos la imagen\n",
    "    image_extracted = tf.reshape(tf.slice(record_bytes, [1], [image_vect_length]), [num_channels, image_height, image_width])\n",
    "    #redimensión de imagen\n",
    "    reshaped_image = tf.transpose(image_extracted, [1,2,0])\n",
    "    reshaped_image = tf.cast(reshaped_image, tf.float32)\n",
    "    #crop aleatorio\n",
    "    final_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, crop_width, crop_height)\n",
    "    if distort_images:\n",
    "        #flip aleatorio horizontal, cambiar brillo y contraste\n",
    "        final_image = tf.image.random_flip_left_right(final_image)\n",
    "        final_image = tf.image.random_brightness(final_image, max_delta=63)\n",
    "        final_image = tf.image.random_contrast(final_image, lower=0.2, upper=1.8)\n",
    "    #estandarización por color\n",
    "    final_image = tf.image.per_image_standardization(final_image)\n",
    "    return final_image, image_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pipeline(batch_size, train_logical=True):\n",
    "    if train_logical:\n",
    "        files = [os.path.join(data_dir, data_folder, 'data_batch_{}.bin'.format(i)) for i in range(1,6)]\n",
    "    else:\n",
    "        files = [os.path.join(data_dir, data_folder, 'test_batch.bin')]\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer(files)\n",
    "    image, label = read_cifar_files(filename_queue)\n",
    "    \n",
    "    min_after_dequeue = 1000\n",
    "    capacity = min_after_dequeue+3*batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch([image,label], batch_size, capacity, min_after_dequeue)\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_cnn_model(input_images, batch_size, train_logical=True):\n",
    "    def truncated_normal_var(name, shape, dtype):\n",
    "        return tf.get_variable(name=name, shape=shape, dtype=dtype, \n",
    "                               initializer=tf.truncated_normal_initializer(stddev=0.05))\n",
    "    def zero_var(name, shape, dtype):\n",
    "        return tf.get_variable(name=name, shape=shape, dtype=dtype, \n",
    "                              initializer=tf.constant_initializer(0.0))\n",
    "    ## Primera capa de convolución\n",
    "    with tf.variable_scope(\"conv1\") as scope:\n",
    "        ##Conv de 5x5 para 3 canales con un total de 64 nodos al final\n",
    "        conv1_kernel = truncated_normal_var(name=\"conv_lernel1\", shape=[5,5,3,64], dtype=tf.float32)\n",
    "        conv1 = tf.nn.conv2d(input_images, conv1_kernel, [1,1,1,1],padding=\"SAME\")\n",
    "        conv1_bias = zero_var(name=\"conv_bias1\", shape=[64], dtype=tf.float32)\n",
    "        conv1_add_bias = tf.nn.bias_add(conv1, conv1_bias)\n",
    "        ## Capa de relu\n",
    "        relu_conv1 = tf.nn.relu(conv1_add_bias)\n",
    "    #Max pooling\n",
    "    pool1 = tf.nn.max_pool(relu_conv1, ksize=[1,3,3,1], strides=[1,2,2,1],padding=\"SAME\", name=\"pool_layer1\")\n",
    "    \n",
    "    #Normalización local\n",
    "    norm1 = tf.nn.lrn(pool1, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name=\"norm1\")\n",
    "    \n",
    "    ## Segunda capa de convolución\n",
    "    with tf.variable_scope(\"conv2\") as scope:\n",
    "        ##Conv de 5x5 para 64 nodos de entrada con un total de 64 nodos al final\n",
    "        conv2_kernel = truncated_normal_var(name=\"conv_lernel2\", shape=[5,5,64,64], dtype=tf.float32)\n",
    "        conv2 = tf.nn.conv2d(norm1, conv2_kernel, [1,1,1,1],padding=\"SAME\")\n",
    "        conv2_bias = zero_var(name=\"conv_bias2\", shape=[64], dtype=tf.float32)\n",
    "        conv2_add_bias = tf.nn.bias_add(conv2, conv2_bias)\n",
    "        ## Capa de relu\n",
    "        relu_conv2 = tf.nn.relu(conv2_add_bias)\n",
    "    #Max pooling\n",
    "    pool2 = tf.nn.max_pool(relu_conv2, ksize=[1,3,3,1], strides=[1,2,2,1],padding=\"SAME\", name=\"pool_layer2\")\n",
    "    \n",
    "    #Normalización local\n",
    "    norm2 = tf.nn.lrn(pool2, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name=\"norm2\")\n",
    "    \n",
    "    #Redimensionar a una matriz para poder multiplicar en las capas totalmente conectadas\n",
    "    reshaped_output = tf.reshape(norm2, [batch_size,-1])\n",
    "    reshaped_dim = reshaped_output.get_shape()[1].value\n",
    "    \n",
    "    ##Primera capa totalmente conectada\n",
    "    with tf.variable_scope(\"full1\") as scope:\n",
    "        #Full connected con 384 nodos\n",
    "        full_weight1 = truncated_normal_var(name=\"full_mult1\", shape=[reshaped_dim, 384], dtype = tf.float32)\n",
    "        full_bias1 = zero_var(name=\"full_bias1\", shape=[384], dtype=tf.float32)\n",
    "        full_layer1 = tf.nn.relu(tf.add(tf.matmul(reshaped_output, full_weight1), full_bias1))\n",
    "    ##Segunda capa totalmente conectada\n",
    "    with tf.variable_scope(\"full2\") as scope:\n",
    "        #Full connected con 192 nodos\n",
    "        full_weight2 = truncated_normal_var(name=\"full_mult2\", shape=[384, 192], dtype = tf.float32)\n",
    "        full_bias2 = zero_var(name=\"full_bias2\", shape=[192], dtype=tf.float32)\n",
    "        full_layer2 = tf.nn.relu(tf.add(tf.matmul(full_layer1, full_weight2), full_bias2))\n",
    "    ##Tercera capa totalmente conectada\n",
    "    with tf.variable_scope(\"full3\") as scope:\n",
    "        #Full connected con una de las 10 categorías de salida\n",
    "        full_weight3 = truncated_normal_var(name=\"full_mult3\", shape=[192, num_targets], dtype = tf.float32)\n",
    "        full_bias3 = zero_var(name=\"full_bias3\", shape=[num_targets], dtype=tf.float32)\n",
    "        final_output = tf.add(tf.matmul(full_layer2, full_weight3), full_bias3)\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
