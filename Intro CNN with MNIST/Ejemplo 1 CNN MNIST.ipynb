{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Una red neuronal convolucional simple\n",
    "## 2 Capas de convolucion+ReLU+Max Pooling\n",
    "## 2 Capas totalmente conectadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "ops.reset_default_graph()\n",
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-bfc2b05617dd>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'MNIST_data/'\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xdata = np.array([np.reshape(x, (28,28)) for x in mnist.train.images])\n",
    "test_xdata = np.array([np.reshape(x, (28,28)) for x in mnist.test.images])\n",
    "train_labels = mnist.train.labels\n",
    "test_labels = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.005\n",
    "evaluation_size = 500\n",
    "image_width = train_xdata[0].shape[0]\n",
    "image_height = train_xdata[0].shape[1]\n",
    "target_size = max(train_labels)+1\n",
    "num_chanels = 1\n",
    "generations = 500\n",
    "eval_every = 5\n",
    "conv1_features = 25\n",
    "conv2_features = 50\n",
    "max_pool_size1 = 2\n",
    "max_pool_size2 = 2\n",
    "full_connected_size1 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input_shape = (batch_size, image_width, image_height, num_chanels)\n",
    "x_input = tf.placeholder(tf.float32, shape=x_input_shape)\n",
    "y_target = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "\n",
    "eval_input_shape = (evaluation_size, image_width, image_height, num_chanels)\n",
    "eval_input = tf.placeholder(tf.float32, shape=eval_input_shape)\n",
    "eval_target = tf.placeholder(tf.float32, shape=(evaluation_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_weight = tf.Variable(tf.truncated_normal([4,4, num_chanels, conv1_features], stddev=0.1, dtype=tf.float32))\n",
    "conv1_bias = tf.Variable(tf.zeros([conv1_features], dtype=tf.float32))\n",
    "\n",
    "conv2_weight = tf.Variable(tf.truncated_normal([4,4, conv1_features, conv2_features], stddev=0.1, dtype=tf.float32))\n",
    "conv2_bias = tf.Variable(tf.zeros([conv2_features], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_width = image_width // (max_pool_size1*max_pool_size2)\n",
    "resulting_height = image_height // (max_pool_size1*max_pool_size2)\n",
    "\n",
    "full1_input_size = resulting_width*resulting_height*conv2_features\n",
    "full1_weight = tf.Variable(tf.truncated_normal([full1_input_size, full_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "full1_bias = tf.Variable(tf.truncated_normal([full_connected_size1], stddev=0.1, dtype = tf.float32))\n",
    "\n",
    "full2_weight = tf.Variable(tf.truncated_normal([full_connected_size1, target_size], stddev=0.1, dtype=tf.float32))\n",
    "full2_bias = tf.Variable(tf.truncated_normal([target_size], stddev=0.1, dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_conv_neural_net(input_data):\n",
    "    ## Primera capa Conv+ReLU+Maxpool\n",
    "    conv1 = tf.nn.conv2d(input_data, conv1_weight, strides=[1,1,1,1], padding=\"SAME\")\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "    max_pool1 = tf.nn.max_pool(relu1, ksize=[1,max_pool_size1, max_pool_size1,1], \n",
    "                               strides=[1, max_pool_size1, max_pool_size1,1], padding=\"SAME\")\n",
    "    ## Segunda capa Conv+ReLU+Maxpool\n",
    "    conv2 = tf.nn.conv2d(max_pool1, conv2_weight, strides=[1,1,1,1], padding=\"SAME\")\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))\n",
    "    max_pool2 = tf.nn.max_pool(relu2, ksize=[1,max_pool_size2, max_pool_size2,1], \n",
    "                               strides=[1, max_pool_size2, max_pool_size2,1], padding=\"SAME\")\n",
    "    ## Operación de flattening para aplanar la imagen en un vector\n",
    "    final_conv_shape = max_pool2.get_shape().as_list()\n",
    "    final_shape = final_conv_shape[1]*final_conv_shape[2]*final_conv_shape[3]\n",
    "    flat_output = tf.reshape(max_pool2, [final_conv_shape[0], final_shape])\n",
    "    ## Tercera capa, totalmente conectada\n",
    "    fully_connected_1 = tf.nn.relu(tf.add(tf.matmul(flat_output, full1_weight), full1_bias))\n",
    "    ## Cuarta capa, totalmente conectada\n",
    "    fully_connected_2 = tf.add(tf.matmul(fully_connected_1, full2_weight), full2_bias)\n",
    "    return fully_connected_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ouput = my_conv_neural_net(x_input)\n",
    "test_model_output = my_conv_neural_net(eval_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_ouput, labels = y_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(model_ouput)\n",
    "test_prediction = tf.nn.softmax(test_model_output)\n",
    "\n",
    "def get_accuracy(logits, targets):\n",
    "    batch_predictions = np.argmax(logits, axis = 1)\n",
    "    num_corrects = np.sum(np.equal(batch_predictions, targets))\n",
    "    return 100.0*num_corrects/batch_predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_optim = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "train_step = my_optim.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración 5. Train Loss: 2.233. Train Acc: 11.000. Test Acc: 10.800\n",
      "Iteración 10. Train Loss: 2.162. Train Acc: 22.000. Test Acc: 27.400\n",
      "Iteración 15. Train Loss: 2.017. Train Acc: 28.000. Test Acc: 37.200\n",
      "Iteración 20. Train Loss: 1.787. Train Acc: 48.000. Test Acc: 48.200\n",
      "Iteración 25. Train Loss: 1.532. Train Acc: 61.000. Test Acc: 60.000\n",
      "Iteración 30. Train Loss: 1.403. Train Acc: 55.000. Test Acc: 64.400\n",
      "Iteración 35. Train Loss: 1.069. Train Acc: 69.000. Test Acc: 72.200\n",
      "Iteración 40. Train Loss: 0.865. Train Acc: 74.000. Test Acc: 74.600\n",
      "Iteración 45. Train Loss: 0.825. Train Acc: 71.000. Test Acc: 82.200\n",
      "Iteración 50. Train Loss: 0.825. Train Acc: 81.000. Test Acc: 83.000\n",
      "Iteración 55. Train Loss: 0.675. Train Acc: 74.000. Test Acc: 81.000\n",
      "Iteración 60. Train Loss: 0.581. Train Acc: 80.000. Test Acc: 85.400\n",
      "Iteración 65. Train Loss: 0.381. Train Acc: 86.000. Test Acc: 90.600\n",
      "Iteración 70. Train Loss: 0.420. Train Acc: 84.000. Test Acc: 89.400\n",
      "Iteración 75. Train Loss: 0.279. Train Acc: 92.000. Test Acc: 88.000\n",
      "Iteración 80. Train Loss: 0.408. Train Acc: 89.000. Test Acc: 87.800\n",
      "Iteración 85. Train Loss: 0.357. Train Acc: 92.000. Test Acc: 91.200\n",
      "Iteración 90. Train Loss: 0.352. Train Acc: 88.000. Test Acc: 90.600\n",
      "Iteración 95. Train Loss: 0.317. Train Acc: 88.000. Test Acc: 90.600\n",
      "Iteración 100. Train Loss: 0.362. Train Acc: 91.000. Test Acc: 92.800\n",
      "Iteración 105. Train Loss: 0.243. Train Acc: 94.000. Test Acc: 93.800\n",
      "Iteración 110. Train Loss: 0.346. Train Acc: 89.000. Test Acc: 92.800\n",
      "Iteración 115. Train Loss: 0.411. Train Acc: 85.000. Test Acc: 93.000\n",
      "Iteración 120. Train Loss: 0.246. Train Acc: 94.000. Test Acc: 90.600\n",
      "Iteración 125. Train Loss: 0.275. Train Acc: 91.000. Test Acc: 92.200\n",
      "Iteración 130. Train Loss: 0.363. Train Acc: 91.000. Test Acc: 92.000\n",
      "Iteración 135. Train Loss: 0.311. Train Acc: 90.000. Test Acc: 92.600\n",
      "Iteración 140. Train Loss: 0.243. Train Acc: 94.000. Test Acc: 92.000\n",
      "Iteración 145. Train Loss: 0.187. Train Acc: 93.000. Test Acc: 92.200\n",
      "Iteración 150. Train Loss: 0.419. Train Acc: 89.000. Test Acc: 92.000\n",
      "Iteración 155. Train Loss: 0.250. Train Acc: 93.000. Test Acc: 92.400\n",
      "Iteración 160. Train Loss: 0.200. Train Acc: 93.000. Test Acc: 89.800\n",
      "Iteración 165. Train Loss: 0.201. Train Acc: 93.000. Test Acc: 92.200\n",
      "Iteración 170. Train Loss: 0.165. Train Acc: 95.000. Test Acc: 92.800\n",
      "Iteración 175. Train Loss: 0.198. Train Acc: 92.000. Test Acc: 93.800\n",
      "Iteración 180. Train Loss: 0.306. Train Acc: 90.000. Test Acc: 92.600\n",
      "Iteración 185. Train Loss: 0.276. Train Acc: 89.000. Test Acc: 91.400\n",
      "Iteración 190. Train Loss: 0.252. Train Acc: 93.000. Test Acc: 91.600\n",
      "Iteración 195. Train Loss: 0.371. Train Acc: 88.000. Test Acc: 90.600\n",
      "Iteración 200. Train Loss: 0.242. Train Acc: 92.000. Test Acc: 92.600\n",
      "Iteración 205. Train Loss: 0.263. Train Acc: 91.000. Test Acc: 93.400\n",
      "Iteración 210. Train Loss: 0.191. Train Acc: 96.000. Test Acc: 92.400\n",
      "Iteración 215. Train Loss: 0.273. Train Acc: 94.000. Test Acc: 92.800\n",
      "Iteración 220. Train Loss: 0.278. Train Acc: 92.000. Test Acc: 92.600\n",
      "Iteración 225. Train Loss: 0.243. Train Acc: 92.000. Test Acc: 93.600\n",
      "Iteración 230. Train Loss: 0.209. Train Acc: 95.000. Test Acc: 94.200\n",
      "Iteración 235. Train Loss: 0.222. Train Acc: 92.000. Test Acc: 93.600\n",
      "Iteración 240. Train Loss: 0.201. Train Acc: 92.000. Test Acc: 94.400\n",
      "Iteración 245. Train Loss: 0.296. Train Acc: 89.000. Test Acc: 94.000\n",
      "Iteración 250. Train Loss: 0.206. Train Acc: 93.000. Test Acc: 94.800\n",
      "Iteración 255. Train Loss: 0.151. Train Acc: 93.000. Test Acc: 94.200\n",
      "Iteración 260. Train Loss: 0.151. Train Acc: 94.000. Test Acc: 94.200\n",
      "Iteración 265. Train Loss: 0.152. Train Acc: 96.000. Test Acc: 95.000\n",
      "Iteración 270. Train Loss: 0.119. Train Acc: 96.000. Test Acc: 95.600\n",
      "Iteración 275. Train Loss: 0.201. Train Acc: 93.000. Test Acc: 95.600\n",
      "Iteración 280. Train Loss: 0.147. Train Acc: 95.000. Test Acc: 95.000\n",
      "Iteración 285. Train Loss: 0.195. Train Acc: 95.000. Test Acc: 94.400\n",
      "Iteración 290. Train Loss: 0.116. Train Acc: 96.000. Test Acc: 96.600\n",
      "Iteración 295. Train Loss: 0.129. Train Acc: 96.000. Test Acc: 92.600\n",
      "Iteración 300. Train Loss: 0.341. Train Acc: 89.000. Test Acc: 96.800\n",
      "Iteración 305. Train Loss: 0.140. Train Acc: 97.000. Test Acc: 96.800\n",
      "Iteración 310. Train Loss: 0.185. Train Acc: 94.000. Test Acc: 95.400\n",
      "Iteración 315. Train Loss: 0.245. Train Acc: 91.000. Test Acc: 95.200\n",
      "Iteración 320. Train Loss: 0.191. Train Acc: 95.000. Test Acc: 94.800\n",
      "Iteración 325. Train Loss: 0.081. Train Acc: 98.000. Test Acc: 95.400\n",
      "Iteración 330. Train Loss: 0.163. Train Acc: 96.000. Test Acc: 96.000\n",
      "Iteración 335. Train Loss: 0.110. Train Acc: 97.000. Test Acc: 95.600\n",
      "Iteración 340. Train Loss: 0.127. Train Acc: 97.000. Test Acc: 95.200\n",
      "Iteración 345. Train Loss: 0.198. Train Acc: 96.000. Test Acc: 97.600\n",
      "Iteración 350. Train Loss: 0.097. Train Acc: 97.000. Test Acc: 97.200\n",
      "Iteración 355. Train Loss: 0.121. Train Acc: 96.000. Test Acc: 96.800\n",
      "Iteración 360. Train Loss: 0.129. Train Acc: 96.000. Test Acc: 94.000\n",
      "Iteración 365. Train Loss: 0.226. Train Acc: 94.000. Test Acc: 96.800\n",
      "Iteración 370. Train Loss: 0.080. Train Acc: 98.000. Test Acc: 96.600\n",
      "Iteración 375. Train Loss: 0.155. Train Acc: 95.000. Test Acc: 97.200\n",
      "Iteración 380. Train Loss: 0.091. Train Acc: 96.000. Test Acc: 97.800\n",
      "Iteración 385. Train Loss: 0.218. Train Acc: 92.000. Test Acc: 94.600\n",
      "Iteración 390. Train Loss: 0.165. Train Acc: 94.000. Test Acc: 94.600\n",
      "Iteración 395. Train Loss: 0.107. Train Acc: 98.000. Test Acc: 95.600\n",
      "Iteración 400. Train Loss: 0.101. Train Acc: 98.000. Test Acc: 94.800\n",
      "Iteración 405. Train Loss: 0.176. Train Acc: 95.000. Test Acc: 95.400\n",
      "Iteración 410. Train Loss: 0.057. Train Acc: 100.000. Test Acc: 96.200\n",
      "Iteración 415. Train Loss: 0.113. Train Acc: 98.000. Test Acc: 96.000\n",
      "Iteración 420. Train Loss: 0.212. Train Acc: 91.000. Test Acc: 95.200\n",
      "Iteración 425. Train Loss: 0.180. Train Acc: 95.000. Test Acc: 96.600\n",
      "Iteración 430. Train Loss: 0.061. Train Acc: 99.000. Test Acc: 95.200\n",
      "Iteración 435. Train Loss: 0.131. Train Acc: 97.000. Test Acc: 94.800\n",
      "Iteración 440. Train Loss: 0.066. Train Acc: 97.000. Test Acc: 95.200\n",
      "Iteración 445. Train Loss: 0.203. Train Acc: 92.000. Test Acc: 95.200\n",
      "Iteración 450. Train Loss: 0.073. Train Acc: 99.000. Test Acc: 96.000\n",
      "Iteración 455. Train Loss: 0.083. Train Acc: 97.000. Test Acc: 96.600\n",
      "Iteración 460. Train Loss: 0.171. Train Acc: 97.000. Test Acc: 96.000\n",
      "Iteración 465. Train Loss: 0.072. Train Acc: 97.000. Test Acc: 96.200\n",
      "Iteración 470. Train Loss: 0.148. Train Acc: 93.000. Test Acc: 96.200\n",
      "Iteración 475. Train Loss: 0.111. Train Acc: 97.000. Test Acc: 95.800\n",
      "Iteración 480. Train Loss: 0.117. Train Acc: 99.000. Test Acc: 97.000\n",
      "Iteración 485. Train Loss: 0.109. Train Acc: 96.000. Test Acc: 95.800\n",
      "Iteración 490. Train Loss: 0.183. Train Acc: 93.000. Test Acc: 97.200\n",
      "Iteración 495. Train Loss: 0.058. Train Acc: 99.000. Test Acc: 95.400\n",
      "Iteración 500. Train Loss: 0.109. Train Acc: 96.000. Test Acc: 97.000\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "i_vals = []\n",
    "for i in range(generations):\n",
    "    rand_idx = np.random.choice(len(train_xdata), size = batch_size)\n",
    "    rand_x = train_xdata[rand_idx]\n",
    "    rand_x = np.expand_dims(rand_x, 3)\n",
    "    rand_y = train_labels[rand_idx]\n",
    "    train_dict = {x_input:rand_x, y_target:rand_y}\n",
    "    session.run(train_step, feed_dict=train_dict)\n",
    "    temp_train_loss, temp_train_preds = session.run([loss, prediction], feed_dict=train_dict)\n",
    "    temp_train_acc = get_accuracy(temp_train_preds, rand_y)\n",
    "    \n",
    "    if(i+1) % eval_every == 0:\n",
    "        rand_idx_eval = np.random.choice(len(test_xdata), size = evaluation_size)\n",
    "        rand_x_eval = test_xdata[rand_idx_eval]\n",
    "        rand_x_eval = np.expand_dims(rand_x_eval, 3)\n",
    "        rand_y_eval = test_labels[rand_idx_eval]\n",
    "        test_dict = {eval_input:rand_x_eval, eval_target:rand_y_eval}\n",
    "\n",
    "        temp_test_preds = session.run( test_prediction, feed_dict=test_dict)\n",
    "        temp_test_acc = get_accuracy(temp_test_preds, rand_y_eval)\n",
    "        \n",
    "        i_vals.append(i+1)\n",
    "        train_loss.append(temp_train_loss)\n",
    "        train_acc.append(temp_train_acc)\n",
    "        test_acc.append(temp_test_acc)\n",
    " \n",
    "        acc_and_loss = [(i+1),temp_train_loss, temp_train_acc, temp_test_acc]\n",
    "        acc_and_loss = [np.round(x,3) for x in acc_and_loss]\n",
    "        print(\"Iteración {}. Train Loss: {:.3f}. Train Acc: {:.3f}. Test Acc: {:.3f}\".format(*acc_and_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluar la calidad de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(i_vals, train_loss, 'k-')\n",
    "plt.title(\"Softmax Loss para cada Iteración\")\n",
    "plt.xlabel(\"Iteración\")\n",
    "plt.ylabel(\"Pérdida Softmax\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(i_vals, train_acc, 'r-', label=\"Precisión en entrenamiento\")\n",
    "plt.plot(i_vals, test_acc, 'b--', label=\"Precisión en testing\")\n",
    "plt.xlabel(\"Iteración\")\n",
    "plt.ylabel(\"Precisión\")\n",
    "plt.ylim([0,100])\n",
    "plt.title(\"Precisión en la predicción\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = rand_y[0:6]\n",
    "predictions = np.argmax(temp_train_preds, axis=1)[0:6]\n",
    "images = np.squeeze(rand_x[0:6])\n",
    "nrows = 2\n",
    "ncols = 3\n",
    "for i in range(6):\n",
    "    plt.subplot(nrows, ncols, i+1)\n",
    "    plt.imshow(np.reshape(images[i], [28,28]), cmap=\"Greys_r\")\n",
    "    plt.title(\"Actual \"+str(actuals[i])+\" Predicción: \"+str(predictions[i]), fontsize = 10)\n",
    "    frame=plt.gca()\n",
    "    frame.axes.get_xaxis().set_visible(False)\n",
    "    frame.axes.get_yaxis().set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
