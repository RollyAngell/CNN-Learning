{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Una red neuronal convolucional simple\n",
    "## 2 Capas de convolucion+ReLU+Max Pooling\n",
    "## 2 Capas totalmente conectadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "ops.reset_default_graph()\n",
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-bfc2b05617dd>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'MNIST_data/'\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xdata = np.array([np.reshape(x, (28,28)) for x in mnist.train.images])\n",
    "test_xdata = np.array([np.reshape(x, (28,28)) for x in mnist.test.images])\n",
    "train_labels = mnist.train.labels\n",
    "test_labels = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.005\n",
    "evaluation_size = 500\n",
    "image_width = train_xdata[0].shape[0]\n",
    "image_height = train_xdata[0].shape[1]\n",
    "target_size = max(train_labels)+1\n",
    "num_chanels = 1\n",
    "generations = 500\n",
    "eval_every = 5\n",
    "conv1_features = 25\n",
    "conv2_features = 50\n",
    "max_pool_size1 = 2\n",
    "max_pool_size2 = 2\n",
    "full_connected_size1 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input_shape = (batch_size, image_width, image_height, num_chanels)\n",
    "x_input = tf.placeholder(tf.float32, shape=x_input_shape)\n",
    "y_target = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "\n",
    "eval_input_shape = (evaluation_size, image_width, image_height, num_chanels)\n",
    "eval_input = tf.placeholder(tf.float32, shape=eval_input_shape)\n",
    "eval_target = tf.placeholder(tf.float32, shape=(evaluation_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_weight = tf.Variable(tf.truncated_normal([4,4, num_chanels, conv1_features], stddev=0.1, dtype=tf.float32))\n",
    "conv1_bias = tf.Variable(tf.zeros([conv1_features], dtype=tf.float32))\n",
    "\n",
    "conv2_weight = tf.Variable(tf.truncated_normal([4,4, conv1_features, conv2_features], stddev=0.1, dtype=tf.float32))\n",
    "conv2_bias = tf.Variable(tf.zeros([conv2_features], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_width = image_width // (max_pool_size1*max_pool_size2)\n",
    "resulting_height = image_height // (max_pool_size1*max_pool_size2)\n",
    "\n",
    "full1_input_size = resulting_width*resulting_height*conv2_features\n",
    "full1_weight = tf.Variable(tf.truncated_normal([full1_input_size, full_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "full1_bias = tf.Variable(tf.truncated_normal([full_connected_size1], stddev=0.1, dtype = tf.float32))\n",
    "\n",
    "full2_weight = tf.Variable(tf.truncated_normal([full_connected_size1, target_size], stddev=0.1, dtype=tf.float32))\n",
    "full2_bias = tf.Variable(tf.truncated_normal([target_size], stddev=0.1, dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_conv_neural_net(input_data):\n",
    "    ## Primera capa Conv+ReLU+Maxpool\n",
    "    conv1 = tf.nn.conv2d(input_data, conv1_weight, strides=[1,1,1,1], padding=\"SAME\")\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "    max_pool1 = tf.nn.max_pool(relu1, ksize=[1,max_pool_size1, max_pool_size1,1], \n",
    "                               strides=[1, max_pool_size1, max_pool_size1,1], padding=\"SAME\")\n",
    "    ## Segunda capa Conv+ReLU+Maxpool\n",
    "    conv2 = tf.nn.conv2d(max_pool1, conv2_weight, strides=[1,1,1,1], padding=\"SAME\")\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))\n",
    "    max_pool2 = tf.nn.max_pool(relu2, ksize=[1,max_pool_size2, max_pool_size2,1], \n",
    "                               strides=[1, max_pool_size2, max_pool_size2,1], padding=\"SAME\")\n",
    "    ## Operaci√≥n de flattening para aplanar la imagen en un vector\n",
    "    final_conv_shape = max_pool2.get_shape().as_list()\n",
    "    final_shape = final_conv_shape[1]*final_conv_shape[2]*final_conv_shape[3]\n",
    "    flat_output = tf.reshape(max_pool2, [final_conv_shape[0], final_shape])\n",
    "    ## Tercera capa, totalmente conectada\n",
    "    fully_connected_1 = tf.nn.relu(tf.add(tf.matmul(flat_output, full1_weight), full1_bias))\n",
    "    ## Cuarta capa, totalmente conectada\n",
    "    fully_connected_2 = tf.add(tf.matmul(fully_connected_1, full2_weight), full2_bias)\n",
    "    return fully_connected_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ouput = my_conv_neural_net(x_input)\n",
    "test_model_output = my_conv_neural_net(eval_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_ouput, labels = y_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(model_ouput)\n",
    "test_prediction = tf.nn.softmax(test_model_output)\n",
    "\n",
    "def get_accuracy(logits, targets):\n",
    "    batch_predictions = np.argmax(logits, axis = 1)\n",
    "    num_corrects = np.sum(np.equal(batch_predictions, targets))\n",
    "    return 100.0*num_corrects/batch_predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
