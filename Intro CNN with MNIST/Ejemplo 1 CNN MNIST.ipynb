{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Una red neuronal convolucional simple\n",
    "## 2 Capas de convolucion+ReLU+Max Pooling\n",
    "## 2 Capas totalmente conectadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "ops.reset_default_graph()\n",
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-bfc2b05617dd>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'MNIST_data/'\n",
    "mnist = input_data.read_data_sets(data_dir, one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xdata = np.array([np.reshape(x, (28,28)) for x in mnist.train.images])\n",
    "test_xdata = np.array([np.reshape(x, (28,28)) for x in mnist.test.images])\n",
    "train_labels = mnist.train.labels\n",
    "test_labels = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.005\n",
    "evaluation_size = 500\n",
    "image_width = train_xdata[0].shape[0]\n",
    "image_height = train_xdata[0].shape[1]\n",
    "target_size = max(train_labels)+1\n",
    "num_chanels = 1\n",
    "generations = 500\n",
    "eval_every = 5\n",
    "conv1_features = 25\n",
    "conv2_features = 50\n",
    "max_pool_size1 = 2\n",
    "max_pool_size2 = 2\n",
    "full_connected_size1 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input_shape = (batch_size, image_width, image_height, num_chanels)\n",
    "x_input = tf.placeholder(tf.float32, shape=x_input_shape)\n",
    "y_target = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "\n",
    "eval_input_shape = (evaluation_size, image_width, image_height, num_chanels)\n",
    "eval_input = tf.placeholder(tf.float32, shape=eval_input_shape)\n",
    "eval_target = tf.placeholder(tf.float32, shape=(evaluation_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_weight = tf.Variable(tf.truncated_normal([4,4, num_chanels, conv1_features], stddev=0.1, dtype=tf.float32))\n",
    "conv1_bias = tf.Variable(tf.zeros([conv1_features], dtype=tf.float32))\n",
    "\n",
    "conv2_weight = tf.Variable(tf.truncated_normal([4,4, conv1_features, conv2_features], stddev=0.1, dtype=tf.float32))\n",
    "conv2_bias = tf.Variable(tf.zeros([conv2_features], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_width = image_width // (max_pool_size1*max_pool_size2)\n",
    "resulting_height = image_height // (max_pool_size1*max_pool_size2)\n",
    "\n",
    "full1_input_size = resulting_width*resulting_height*conv2_features\n",
    "full1_weight = tf.Variable(tf.truncated_normal([full1_input_size, full_connected_size1], stddev=0.1, dtype=tf.float32))\n",
    "full1_bias = tf.Variable(tf.truncated_normal([full_connected_size1], stddev=0.1, dtype = tf.float32))\n",
    "\n",
    "full2_weight = tf.Variable(tf.truncated_normal([full_connected_size1, target_size], stddev=0.1, dtype=tf.float32))\n",
    "full2_bias = tf.Variable(tf.truncated_normal([target_size], stddev=0.1, dtype=tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_conv_neural_net(input_data):\n",
    "    ## Primera capa Conv+ReLU+Maxpool\n",
    "    conv1 = tf.nn.conv2d(input_data, conv1_weight, strides=[1,1,1,1], padding=\"SAME\")\n",
    "    relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "    max_pool1 = tf.nn.max_pool(relu1, ksize=[1,max_pool_size1, max_pool_size1,1], \n",
    "                               strides=[1, max_pool_size1, max_pool_size1,1], padding=\"SAME\")\n",
    "    ## Segunda capa Conv+ReLU+Maxpool\n",
    "    conv2 = tf.nn.conv2d(max_pool1, conv2_weight, strides=[1,1,1,1], padding=\"SAME\")\n",
    "    relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))\n",
    "    max_pool2 = tf.nn.max_pool(relu2, ksize=[1,max_pool_size2, max_pool_size2,1], \n",
    "                               strides=[1, max_pool_size2, max_pool_size2,1], padding=\"SAME\")\n",
    "    ## Operación de flattening para aplanar la imagen en un vector\n",
    "    final_conv_shape = max_pool2.get_shape().as_list()\n",
    "    final_shape = final_conv_shape[1]*final_conv_shape[2]*final_conv_shape[3]\n",
    "    flat_output = tf.reshape(max_pool2, [final_conv_shape[0], final_shape])\n",
    "    ## Tercera capa, totalmente conectada\n",
    "    fully_connected_1 = tf.nn.relu(tf.add(tf.matmul(flat_output, full1_weight), full1_bias))\n",
    "    ## Cuarta capa, totalmente conectada\n",
    "    fully_connected_2 = tf.add(tf.matmul(fully_connected_1, full2_weight), full2_bias)\n",
    "    return fully_connected_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ouput = my_conv_neural_net(x_input)\n",
    "test_model_output = my_conv_neural_net(eval_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_ouput, labels = y_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(model_ouput)\n",
    "test_prediction = tf.nn.softmax(test_model_output)\n",
    "\n",
    "def get_accuracy(logits, targets):\n",
    "    batch_predictions = np.argmax(logits, axis = 1)\n",
    "    num_corrects = np.sum(np.equal(batch_predictions, targets))\n",
    "    return 100.0*num_corrects/batch_predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_optim = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "train_step = my_optim.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración 5. Train Loss: 2.393. Train Acc: 6.000. Test Acc: 8.000\n",
      "Iteración 10. Train Loss: 2.256. Train Acc: 12.000. Test Acc: 21.000\n",
      "Iteración 15. Train Loss: 2.073. Train Acc: 40.000. Test Acc: 35.400\n",
      "Iteración 20. Train Loss: 1.918. Train Acc: 45.000. Test Acc: 51.000\n",
      "Iteración 25. Train Loss: 1.773. Train Acc: 52.000. Test Acc: 59.400\n",
      "Iteración 30. Train Loss: 1.498. Train Acc: 63.000. Test Acc: 66.800\n",
      "Iteración 35. Train Loss: 1.183. Train Acc: 70.000. Test Acc: 68.800\n",
      "Iteración 40. Train Loss: 0.896. Train Acc: 74.000. Test Acc: 72.200\n",
      "Iteración 45. Train Loss: 0.763. Train Acc: 76.000. Test Acc: 80.200\n",
      "Iteración 50. Train Loss: 0.835. Train Acc: 75.000. Test Acc: 78.800\n",
      "Iteración 55. Train Loss: 0.634. Train Acc: 78.000. Test Acc: 84.400\n",
      "Iteración 60. Train Loss: 0.413. Train Acc: 89.000. Test Acc: 84.400\n",
      "Iteración 65. Train Loss: 0.330. Train Acc: 90.000. Test Acc: 86.800\n",
      "Iteración 70. Train Loss: 0.371. Train Acc: 90.000. Test Acc: 87.200\n",
      "Iteración 75. Train Loss: 0.495. Train Acc: 85.000. Test Acc: 90.000\n",
      "Iteración 80. Train Loss: 0.442. Train Acc: 87.000. Test Acc: 88.400\n",
      "Iteración 85. Train Loss: 0.434. Train Acc: 86.000. Test Acc: 92.400\n",
      "Iteración 90. Train Loss: 0.429. Train Acc: 89.000. Test Acc: 89.800\n",
      "Iteración 95. Train Loss: 0.236. Train Acc: 93.000. Test Acc: 91.000\n",
      "Iteración 100. Train Loss: 0.238. Train Acc: 95.000. Test Acc: 90.600\n",
      "Iteración 105. Train Loss: 0.463. Train Acc: 86.000. Test Acc: 89.800\n",
      "Iteración 110. Train Loss: 0.364. Train Acc: 85.000. Test Acc: 92.600\n",
      "Iteración 115. Train Loss: 0.358. Train Acc: 88.000. Test Acc: 88.800\n",
      "Iteración 120. Train Loss: 0.344. Train Acc: 88.000. Test Acc: 90.800\n",
      "Iteración 125. Train Loss: 0.287. Train Acc: 91.000. Test Acc: 89.200\n",
      "Iteración 130. Train Loss: 0.313. Train Acc: 93.000. Test Acc: 89.600\n",
      "Iteración 135. Train Loss: 0.477. Train Acc: 84.000. Test Acc: 90.000\n",
      "Iteración 140. Train Loss: 0.314. Train Acc: 90.000. Test Acc: 93.800\n",
      "Iteración 145. Train Loss: 0.188. Train Acc: 95.000. Test Acc: 90.600\n",
      "Iteración 150. Train Loss: 0.265. Train Acc: 92.000. Test Acc: 91.800\n",
      "Iteración 155. Train Loss: 0.230. Train Acc: 91.000. Test Acc: 90.800\n",
      "Iteración 160. Train Loss: 0.287. Train Acc: 91.000. Test Acc: 93.000\n",
      "Iteración 165. Train Loss: 0.147. Train Acc: 96.000. Test Acc: 93.000\n",
      "Iteración 170. Train Loss: 0.319. Train Acc: 91.000. Test Acc: 94.000\n",
      "Iteración 175. Train Loss: 0.121. Train Acc: 95.000. Test Acc: 93.200\n",
      "Iteración 180. Train Loss: 0.182. Train Acc: 96.000. Test Acc: 92.200\n",
      "Iteración 185. Train Loss: 0.156. Train Acc: 96.000. Test Acc: 91.400\n",
      "Iteración 190. Train Loss: 0.311. Train Acc: 89.000. Test Acc: 93.200\n",
      "Iteración 195. Train Loss: 0.204. Train Acc: 91.000. Test Acc: 92.600\n",
      "Iteración 200. Train Loss: 0.191. Train Acc: 95.000. Test Acc: 91.400\n",
      "Iteración 205. Train Loss: 0.364. Train Acc: 91.000. Test Acc: 94.800\n",
      "Iteración 210. Train Loss: 0.191. Train Acc: 97.000. Test Acc: 92.600\n",
      "Iteración 215. Train Loss: 0.215. Train Acc: 93.000. Test Acc: 91.600\n",
      "Iteración 220. Train Loss: 0.261. Train Acc: 92.000. Test Acc: 93.000\n",
      "Iteración 225. Train Loss: 0.236. Train Acc: 92.000. Test Acc: 94.800\n",
      "Iteración 230. Train Loss: 0.194. Train Acc: 92.000. Test Acc: 94.600\n",
      "Iteración 235. Train Loss: 0.233. Train Acc: 93.000. Test Acc: 94.800\n",
      "Iteración 240. Train Loss: 0.287. Train Acc: 92.000. Test Acc: 93.800\n",
      "Iteración 245. Train Loss: 0.224. Train Acc: 93.000. Test Acc: 92.600\n",
      "Iteración 250. Train Loss: 0.193. Train Acc: 92.000. Test Acc: 94.400\n",
      "Iteración 255. Train Loss: 0.175. Train Acc: 94.000. Test Acc: 95.000\n",
      "Iteración 260. Train Loss: 0.240. Train Acc: 91.000. Test Acc: 91.400\n",
      "Iteración 265. Train Loss: 0.155. Train Acc: 94.000. Test Acc: 94.000\n",
      "Iteración 270. Train Loss: 0.149. Train Acc: 95.000. Test Acc: 92.400\n",
      "Iteración 275. Train Loss: 0.277. Train Acc: 88.000. Test Acc: 94.400\n",
      "Iteración 280. Train Loss: 0.181. Train Acc: 95.000. Test Acc: 94.400\n",
      "Iteración 285. Train Loss: 0.153. Train Acc: 96.000. Test Acc: 94.800\n",
      "Iteración 290. Train Loss: 0.139. Train Acc: 95.000. Test Acc: 94.200\n",
      "Iteración 295. Train Loss: 0.161. Train Acc: 95.000. Test Acc: 90.800\n",
      "Iteración 300. Train Loss: 0.144. Train Acc: 93.000. Test Acc: 96.600\n",
      "Iteración 305. Train Loss: 0.210. Train Acc: 94.000. Test Acc: 93.400\n",
      "Iteración 310. Train Loss: 0.198. Train Acc: 93.000. Test Acc: 95.200\n",
      "Iteración 315. Train Loss: 0.110. Train Acc: 98.000. Test Acc: 94.800\n",
      "Iteración 320. Train Loss: 0.249. Train Acc: 91.000. Test Acc: 96.200\n",
      "Iteración 325. Train Loss: 0.199. Train Acc: 93.000. Test Acc: 93.800\n",
      "Iteración 330. Train Loss: 0.173. Train Acc: 95.000. Test Acc: 96.200\n",
      "Iteración 335. Train Loss: 0.125. Train Acc: 96.000. Test Acc: 94.200\n",
      "Iteración 340. Train Loss: 0.214. Train Acc: 96.000. Test Acc: 95.400\n",
      "Iteración 345. Train Loss: 0.084. Train Acc: 97.000. Test Acc: 95.200\n",
      "Iteración 350. Train Loss: 0.087. Train Acc: 98.000. Test Acc: 95.800\n",
      "Iteración 355. Train Loss: 0.110. Train Acc: 96.000. Test Acc: 96.200\n",
      "Iteración 360. Train Loss: 0.138. Train Acc: 96.000. Test Acc: 95.600\n",
      "Iteración 365. Train Loss: 0.184. Train Acc: 94.000. Test Acc: 95.600\n",
      "Iteración 370. Train Loss: 0.139. Train Acc: 95.000. Test Acc: 96.000\n",
      "Iteración 375. Train Loss: 0.257. Train Acc: 93.000. Test Acc: 96.000\n",
      "Iteración 380. Train Loss: 0.253. Train Acc: 92.000. Test Acc: 96.200\n",
      "Iteración 385. Train Loss: 0.058. Train Acc: 99.000. Test Acc: 94.600\n",
      "Iteración 390. Train Loss: 0.115. Train Acc: 97.000. Test Acc: 95.600\n",
      "Iteración 395. Train Loss: 0.085. Train Acc: 99.000. Test Acc: 95.400\n",
      "Iteración 400. Train Loss: 0.127. Train Acc: 96.000. Test Acc: 96.000\n",
      "Iteración 405. Train Loss: 0.046. Train Acc: 99.000. Test Acc: 96.200\n",
      "Iteración 410. Train Loss: 0.143. Train Acc: 96.000. Test Acc: 95.800\n",
      "Iteración 415. Train Loss: 0.118. Train Acc: 98.000. Test Acc: 95.200\n",
      "Iteración 420. Train Loss: 0.162. Train Acc: 95.000. Test Acc: 94.600\n",
      "Iteración 425. Train Loss: 0.204. Train Acc: 94.000. Test Acc: 96.000\n",
      "Iteración 430. Train Loss: 0.101. Train Acc: 97.000. Test Acc: 94.800\n",
      "Iteración 435. Train Loss: 0.061. Train Acc: 98.000. Test Acc: 95.000\n",
      "Iteración 440. Train Loss: 0.165. Train Acc: 94.000. Test Acc: 94.800\n",
      "Iteración 445. Train Loss: 0.165. Train Acc: 96.000. Test Acc: 95.000\n",
      "Iteración 450. Train Loss: 0.130. Train Acc: 96.000. Test Acc: 96.400\n",
      "Iteración 455. Train Loss: 0.095. Train Acc: 97.000. Test Acc: 95.800\n",
      "Iteración 460. Train Loss: 0.110. Train Acc: 96.000. Test Acc: 95.800\n",
      "Iteración 465. Train Loss: 0.180. Train Acc: 96.000. Test Acc: 94.800\n",
      "Iteración 470. Train Loss: 0.144. Train Acc: 96.000. Test Acc: 95.800\n",
      "Iteración 475. Train Loss: 0.293. Train Acc: 90.000. Test Acc: 96.200\n",
      "Iteración 480. Train Loss: 0.127. Train Acc: 95.000. Test Acc: 95.200\n",
      "Iteración 485. Train Loss: 0.100. Train Acc: 96.000. Test Acc: 93.000\n",
      "Iteración 490. Train Loss: 0.070. Train Acc: 97.000. Test Acc: 95.400\n",
      "Iteración 495. Train Loss: 0.117. Train Acc: 96.000. Test Acc: 93.000\n",
      "Iteración 500. Train Loss: 0.120. Train Acc: 98.000. Test Acc: 96.000\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "i_vals = []\n",
    "for i in range(generations):\n",
    "    rand_idx = np.random.choice(len(train_xdata), size = batch_size)\n",
    "    rand_x = train_xdata[rand_idx]\n",
    "    rand_x = np.expand_dims(rand_x, 3)\n",
    "    rand_y = train_labels[rand_idx]\n",
    "    train_dict = {x_input:rand_x, y_target:rand_y}\n",
    "    session.run(train_step, feed_dict=train_dict)\n",
    "    temp_train_loss, temp_train_preds = session.run([loss, prediction], feed_dict=train_dict)\n",
    "    temp_train_acc = get_accuracy(temp_train_preds, rand_y)\n",
    "    \n",
    "    if(i+1) % eval_every == 0:\n",
    "        rand_idx_eval = np.random.choice(len(test_xdata), size = evaluation_size)\n",
    "        rand_x_eval = test_xdata[rand_idx_eval]\n",
    "        rand_x_eval = np.expand_dims(rand_x_eval, 3)\n",
    "        rand_y_eval = test_labels[rand_idx_eval]\n",
    "        test_dict = {eval_input:rand_x_eval, eval_target:rand_y_eval}\n",
    "\n",
    "        temp_test_preds = session.run( test_prediction, feed_dict=test_dict)\n",
    "        temp_test_acc = get_accuracy(temp_test_preds, rand_y_eval)\n",
    "        \n",
    "        i_vals.append(i+1)\n",
    "        train_loss.append(temp_train_loss)\n",
    "        train_acc.append(temp_train_acc)\n",
    "        test_acc.append(temp_test_acc)\n",
    " \n",
    "        acc_and_loss = [(i+1),temp_train_loss, temp_train_acc, temp_test_acc]\n",
    "        acc_and_loss = [np.round(x,3) for x in acc_and_loss]\n",
    "        print(\"Iteración {}. Train Loss: {:.3f}. Train Acc: {:.3f}. Test Acc: {:.3f}\".format(*acc_and_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
